{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the steps needed for extracting features from the electrodermal activity, skin temperature and accelerometer data similar to the paper as follows: \n",
    "\n",
    "[1] S. Gashi, L. Alecci, E. D. Lascio, M. E. Debus, F. Gasparini and S. Santini, \"The Role of Model Personalization for Sleep Stage and Sleep Quality Recognition Using Wearables,\" in IEEE Pervasive Computing, doi: 10.1109/MPRV.2022.3164334.\n",
    "\n",
    "If you use snippets of this script, please make sure to cite our paper [1], which is available at: https://ieeexplore.ieee.org/document/9768202 \n",
    "\n",
    "Before running the feature extraction steps the signals should be preprocessed. We suggest to follow the preprocessing steps of EDArtifact tool presented in https://github.com/shkurtagashi/EDArtifact/blob/master/EDArtifacts_Detection/EDArtifacts_Detection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required packages\n",
    "\n",
    "Before going further in the data analysis, the required packages for feature extraction should be extracted as follows:\n",
    "\n",
    "- *fftpack*: to compute fast Fourier transform and extract frequency-domain features. \n",
    "- *stats*: to compute statistical features from the time-domain.\n",
    "- *pywt*: to compute the wavelets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from scipy import fftpack\n",
    "from scipy.stats import entropy\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Compute wavelets of the EDA signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wavelets(df):\n",
    "    '''\n",
    "    This function computes the wavelets of the EDA signal.    \n",
    "    INPUT:\n",
    "        df: requires a dataframe with a column named 'EDA_Filtered' \n",
    "            \n",
    "    OUTPUT:\n",
    "        DF: returns the same dataframe the following columns 'Wavelet1', 'Wavelet2', 'Wavelet3'\n",
    "    '''\n",
    "\n",
    "    _, dtw3, dtw2, dtw1 = pywt.wavedec(df['EDA_Filtered'], 'Haar', level=3)\n",
    "    dtw3_duplicates = list(np.repeat(dtw3, 8))\n",
    "    dtw2_duplicates = list(np.repeat(dtw2, 4))\n",
    "    dtw1_duplicates = list(np.repeat(dtw1, 2))\n",
    "\n",
    "    df['Wavelet3'] = dtw3_duplicates[:len(df)]\n",
    "    df['Wavelet2'] = dtw2_duplicates[:len(df)]\n",
    "    df['Wavelet1'] = dtw1_duplicates[:len(df)]\n",
    "    \n",
    "    print(\"1. EDA wavelets computation completed\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Segment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_data(df, segmentation_type):\n",
    "    '''\n",
    "    This function segments the signals into 10-minute non-overlapping windows or computes the features over the whole session/night. \n",
    "    The sampling frequency of our sensors was set to 4HZ, thereby, the window size for segmenting the data is into 10 minutes\n",
    "    is 2400 samples (e.g., 10 minutes*4Hz = 2400 data samples).  \n",
    "    \n",
    "    INPUT:\n",
    "        df: requires a dataframe with columns named: \n",
    "        'Time', 'EDA_Filtered', 'EDA_Phasic', 'EDA_Tonic', 'TEMP_Filtered',\n",
    "        'Wavelet3', 'Wavelet2', 'Wavelet1',\n",
    "       'X_Filtered', 'Y_Filtered', 'Z_Filtered', 'BVP_Filtered', 'User',\n",
    "       'SessionID', 'SleepQuality_label', 'SleepWake_label',\n",
    "       'Artifact'\n",
    "       segmentation_type: refers whether the data should be segmented into 10 minutes windows or by session/night.\n",
    "        \n",
    "    OUTPUT:\n",
    "        df: returns a new dataframe with the following columns\n",
    "        'EDA_Filtered', 'Time', 'Wavelet1', 'Wavelet2', 'Wavelet3', 'EDA_Phasic', 'EDA_Tonic', 'TEMP_Filtered',\n",
    "       'X_Filtered', 'Y_Filtered', 'Z_Filtered', 'BVP_Filtered', 'User',\n",
    "       'SessionID', 'SleepQuality_label', 'SleepWake_label','Artifact'\n",
    "        Each cell in the dataframe contains an array of 2400 values. \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    if segmentation_type == '10min':\n",
    "        window_size = 2400\n",
    "        time = df.Time.values.reshape(-1, window_size)\n",
    "        eda = df.EDA_Filtered.values.reshape(-1, window_size)\n",
    "        eda_phasic = df.EDA_Phasic.values.reshape(-1, window_size)\n",
    "        eda_tonic = df.EDA_Tonic.values.reshape(-1, window_size)\n",
    "        wavelet3 = df.Wavelet3.values.reshape(-1, window_size)\n",
    "        wavelet2 = df.Wavelet2.values.reshape(-1, window_size)\n",
    "        wavelet1 = df.Wavelet1.values.reshape(-1, window_size)\n",
    "        temp = df.TEMP_Filtered.values.reshape(-1, window_size)\n",
    "        x = df.X_Filtered.values.reshape(-1, window_size)\n",
    "        y = df.Y_Filtered.values.reshape(-1, window_size)\n",
    "        z = df.Z_Filtered.values.reshape(-1, window_size)\n",
    "        bvp = df.BVP_Filtered.values.reshape(-1, window_size)\n",
    "        user = df.User.values.reshape(-1, window_size)\n",
    "        sessionID = df.SessionID.values.reshape(-1, window_size)\n",
    "        sleepQuality = df.SleepQuality_label.values.reshape(-1, window_size)\n",
    "        sleepWake = df.SleepWake_label.values.reshape(-1, window_size)\n",
    "        artifact = df.Artifact.values.reshape(-1, window_size)\n",
    "        psqi = df.PSQI.values.reshape(-1, window_size)\n",
    "    else:\n",
    "        time = pd.Series(df.groupby(['User', 'SessionID']).agg({'Time':lambda x: list(x)})['Time'])\n",
    "        eda = pd.Series(df.groupby(['User', 'SessionID']).agg({'EDA_Filtered':lambda x: list(x)})['EDA_Filtered'])\n",
    "        eda_phasic = pd.Series(df.groupby(['User', 'SessionID']).agg({'EDA_Phasic':lambda x: list(x)})['EDA_Phasic'])\n",
    "        eda_tonic = pd.Series(df.groupby(['User', 'SessionID']).agg({'EDA_Tonic':lambda x: list(x)})['EDA_Tonic'])\n",
    "        wavelet3 = pd.Series(df.groupby(['User', 'SessionID']).agg({'Wavelet3':lambda x: list(x)})['Wavelet3'])\n",
    "        wavelet2 = pd.Series(df.groupby(['User', 'SessionID']).agg({'Wavelet2':lambda x: list(x)})['Wavelet2'])\n",
    "        wavelet1 = pd.Series(df.groupby(['User', 'SessionID']).agg({'Wavelet1':lambda x: list(x)})['Wavelet1'])\n",
    "        temp = pd.Series(df.groupby(['User', 'SessionID']).agg({'TEMP_Filtered':lambda x: list(x)})['TEMP_Filtered'])\n",
    "        x = pd.Series(df.groupby(['User', 'SessionID']).agg({'X_Filtered':lambda x: list(x)})['X_Filtered'])\n",
    "        y = pd.Series(df.groupby(['User', 'SessionID']).agg({'Y_Filtered':lambda x: list(x)})['Y_Filtered'])\n",
    "        z = pd.Series(df.groupby(['User', 'SessionID']).agg({'Z_Filtered':lambda x: list(x)})['Z_Filtered'])\n",
    "        bvp = pd.Series(df.groupby(['User', 'SessionID']).agg({'BVP_Filtered':lambda x: list(x)})['BVP_Filtered'])\n",
    "        user = pd.Series(database[['User', 'SessionID']].drop_duplicates()['User'])\n",
    "        sessionID = pd.Series(database[['User', 'SessionID']].drop_duplicates()['SessionID'])\n",
    "        sleepQuality = pd.Series(df.groupby(['User', 'SessionID']).agg({'SleepQuality_label':lambda x: list(x)})['SleepQuality_label'])\n",
    "        sleepWake = pd.Series(df.groupby(['User', 'SessionID']).agg({'SleepWake_label':lambda x: list(x)})['SleepWake_label'])\n",
    "        artifact = pd.Series(df.groupby(['User', 'SessionID']).agg({'Artifact':lambda x: list(x)})['Artifact'])\n",
    "        psqi = pd.Series(df.groupby(['User', 'SessionID']).agg({'PSQI':lambda x: list(x)})['PSQI'])\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(columns=['EDA','Time', 'Wavelet3','Wavelet2','Wavelet1', 'EDA_Phasic', 'EDA_Tonic', \n",
    "                               'TEMP_Filtered','X_Filtered', 'Y_Filtered', 'Z_Filtered', 'BVP_Filtered', 'User',\n",
    "                               'SessionID','SleepQuality_label', 'SleepWake_label', 'Artifact', 'PSQI'])\n",
    "    df.EDA = pd.Series(list(eda))\n",
    "    df.EDA_Phasic = pd.Series(list(eda_phasic))\n",
    "    df.EDA_Tonic = pd.Series(list(eda_tonic))\n",
    "    df.Time = pd.Series(list(time))   \n",
    "    df.Wavelet3 = pd.Series(list(wavelet3))   \n",
    "    df.Wavelet2 = pd.Series(list(wavelet2))   \n",
    "    df.Wavelet1 = pd.Series(list(wavelet1))  \n",
    "    df.TEMP_Filtered = pd.Series(list(temp))  \n",
    "    df.X_Filtered = pd.Series(list(x))  \n",
    "    df.Y_Filtered = pd.Series(list(y))  \n",
    "    df.Z_Filtered = pd.Series(list(z))  \n",
    "    df.BVP_Filtered = pd.Series(list(bvp))  \n",
    "    df.User = pd.Series(list(user))  \n",
    "    df.SessionID = pd.Series(list(sessionID))  \n",
    "    df.SleepQuality_label = pd.Series(list(sleepQuality))  \n",
    "    df.SleepWake_label = pd.Series(list(sleepWake))  \n",
    "    df.Artifact = pd.Series(list(artifact))  \n",
    "    df.PSQI = pd.Series(list(psqi))  \n",
    "\n",
    "    eda_times = chain.from_iterable(zip(*df.Time.values))\n",
    "    eda_times = list(eda_times)\n",
    "    eda_times = eda_times[:len(df)]\n",
    "\n",
    "    df['Time'] = eda_times\n",
    "    \n",
    "    print(\"2. Data segmentation completed\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction \n",
    "\n",
    "\n",
    "We extract features from non-overlapping segments of the signals that could appropriately characterize the sleep-wake and sleep quality events. The three feature groups involve:\n",
    "- **time-domain**: In the time-domain representation of the sig- nal, we extract statistical features, such as mean, standard deviation, standard error, maximum, mini- mum, median, variance, and quantile with a threshold of 0.7. We also count the number of epochs, storms, and artifacts in each window of the EDA signal and used them as features.\n",
    "- **frequency-domain**: We transform each signal into the frequency-domain using fast Fourier transform and extract features in this domain. We get the direct current component (dc), the sum of spectral coefficients, the information entropy, and the energy of the signal.\n",
    "- **time-frequency-domain**: These features include the mean, median, minimum, maximum, quantile, dynamic range, variance, standard error, standard deviation of wavelets coefficients extracted at three different time scales 4, 2, and 1 Hz.\n",
    "\n",
    "We compute these features for each axis of the ACC sensor, TEMP, and EDA signal, as well as from the phasic, and tonic components the EDA signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute *time-domain* features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistical_features(df):\n",
    "    '''\n",
    "    This function computes the time-domain and time-freqency-domain features for each preprocessed \n",
    "    component of the provided signals. The features are calculated over the whole window (e.g., 10 min, or whole night). \n",
    "    \n",
    "    INPUT:\n",
    "        df: requires a dataframe with columns named: \n",
    "        'EDA','Wavelet3','Wavelet2','Wavelet1', 'EDA_Phasic', 'EDA_Tonic', \n",
    "        'TEMP_Filtered','X_Filtered', 'Y_Filtered', 'Z_Filtered', 'BVP_Filtered'\n",
    "        \n",
    "    OUTPUT:\n",
    "        df: returns the same dataframe with the following features calculated for each column defined in columns variable:\n",
    "            median:        median value of the 5 second windo\n",
    "            mean:          average value\n",
    "            std:           standard deviation\n",
    "            var:           variance of the signal\n",
    "            slope:         slope of the signal\n",
    "            min:           minimum value\n",
    "            max:           maximum value \n",
    "            fdmean:        mean of the first derivative\n",
    "            fdstd:         standard deviation of the first derivative\n",
    "            drange:        dynamic range (difference between the maximum and minimum value in the window)\n",
    "            diffs:         difference between the last and first value in the window\n",
    "    '''\n",
    "    \n",
    "    columns = ['EDA','Wavelet3','Wavelet2','Wavelet1', 'EDA_Phasic', 'EDA_Tonic', \n",
    "             'TEMP_Filtered','X_Filtered', 'Y_Filtered', 'Z_Filtered', 'BVP_Filtered']\n",
    "\n",
    "    for col in columns:\n",
    "        data = df[col]\n",
    "        name = col\n",
    "\n",
    "        medians, means, stds, variances, mins, maxs, fdmeans, sdmeans, fdstds, sdstds, dranges, slopes, diffs  = [], [], [], [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "        for i in range(0, len(data)):\n",
    "            eda = data[i]\n",
    "            time = range(1, len(eda)+1)\n",
    "\n",
    "            fd = np.gradient(eda)\n",
    "            fdmeans.append(np.mean(fd))\n",
    "            fdstds.append(np.std(fd))\n",
    "            dranges.append(np.max(eda) - np.min(eda))\n",
    "            medians.append(np.median(eda))\n",
    "            means.append(np.mean(eda))\n",
    "            stds.append(np.std(eda))\n",
    "            variances.append(np.var(eda))\n",
    "            mins.append(np.min(eda))\n",
    "            maxs.append(np.max(eda)) \n",
    "            slope, intercept, r_value, p_value, std_err = linregress(time, data[i])  \n",
    "            slopes.append(slope)\n",
    "            diffs.append(eda[len(eda)-1] - eda[0])\n",
    "\n",
    "\n",
    "        df[name+'_median'] = medians\n",
    "        df[name+'_mean'] = means\n",
    "        df[name+'_std'] = stds\n",
    "        df[name+'_var'] = variances\n",
    "        df[name+'_slope'] = slopes\n",
    "        df[name+'_min'] = mins\n",
    "        df[name+'_max'] = maxs\n",
    "        df[name+'_fdmean'] = fdmeans\n",
    "        df[name+'_fdstd'] = fdstds\n",
    "        df[name+'_drange'] = dranges\n",
    "        df[name+'_diff'] = diffs\n",
    "\n",
    "        \n",
    "    print(\"3.1. Statistical time-domain features extraction completed\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute time-domain features (EDA peaks features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code is adapted from the EDA peak detection script available at: https://github.com/MITMediaLabAffectiveComputing/eda-explorer. If you use this part of the code, please cite the following paper:\n",
    "\n",
    "Taylor, Sara, Natasha Jaques, Weixuan Chen, Szymon Fedor, Akane Sano, and Rosalind Picard. \"Automatic identification of artifacts in electrodermal activity data.\" In 2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 1934-1937. IEEE, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 4\n",
    "def findPeaks(data, offset, start_WT, end_WT, thres=0, sampleRate=SAMPLE_RATE):\n",
    "    '''\n",
    "        This function finds the peaks of an EDA signal and returns basic properties.\n",
    "        Also, peak_end is assumed to be no later than the start of the next peak.\n",
    "        \n",
    "        ********* INPUTS **********\n",
    "        data:        DataFrame with EDA as one of the columns and indexed by a datetimeIndex\n",
    "        offset:      the number of rising samples and falling samples after a peak needed to be counted as a peak\n",
    "        start_WT:    maximum number of seconds before the apex of a peak that is the \"\"start\"\" of the peak\n",
    "        end_WT:      maximum number of seconds after the apex of a peak that is the \"\"rec.t/2\"\" of the peak, 50% of amp\n",
    "        thres:       the minimum uS change required to register as a peak, defaults as 0 (i.e. all peaks count)\n",
    "        sampleRate:  number of samples per second, default=8\n",
    "        \n",
    "        ********* OUTPUTS **********\n",
    "        peaks:               list of binary, 1 if apex of SCR\n",
    "        peak_start:          list of binary, 1 if start of SCR\n",
    "        peak_start_times:    list of strings, if this index is the apex of an SCR, it contains datetime of start of peak\n",
    "        peak_end:            list of binary, 1 if rec.t/2 of SCR\n",
    "        peak_end_times:      list of strings, if this index is the apex of an SCR, it contains datetime of rec.t/2\n",
    "        amplitude:           list of floats,  value of EDA at apex - value of EDA at start\n",
    "        max_deriv:           list of floats, max derivative within 1 second of apex of SCR\n",
    "    '''\n",
    "    \n",
    "    EDA_deriv = data['EDA_Phasic'][1:].values - data['EDA_Phasic'][:-1].values\n",
    "    peaks = np.zeros(len(EDA_deriv))\n",
    "    peak_sign = np.sign(EDA_deriv)\n",
    "    for i in range(int(offset), int(len(EDA_deriv) - offset)):\n",
    "        if peak_sign[i] == 1 and peak_sign[i + 1] < 1:\n",
    "            peaks[i] = 1\n",
    "            for j in range(1, int(offset)):\n",
    "                if peak_sign[i - j] < 1 or peak_sign[i + j] > -1:\n",
    "                    peaks[i] = 0\n",
    "                    break\n",
    "\n",
    "    # Finding start of peaks\n",
    "    peak_start = np.zeros(len(EDA_deriv))\n",
    "    peak_start_times = [''] * len(data)\n",
    "    max_deriv = np.zeros(len(data))\n",
    "    rise_time = np.zeros(len(data))\n",
    "\n",
    "    for i in range(0, len(peaks)):\n",
    "        if peaks[i] == 1:\n",
    "            temp_start = max(0, i - sampleRate)\n",
    "            max_deriv[i] = max(EDA_deriv[temp_start:i])\n",
    "            start_deriv = .01 * max_deriv[i]\n",
    "\n",
    "            found = False\n",
    "            find_start = i\n",
    "            # has to peak within start_WT seconds\n",
    "            while found == False and find_start > (i - start_WT * sampleRate):\n",
    "                if EDA_deriv[find_start] < start_deriv:\n",
    "                    found = True\n",
    "                    peak_start[find_start] = 1\n",
    "                    peak_start_times[i] = data.index[find_start]\n",
    "                    rise_time[i] = get_seconds_and_microseconds(data.index[i] - pd.to_datetime(peak_start_times[i]))\n",
    "\n",
    "                find_start = find_start - 1\n",
    "\n",
    "        # If we didn't find a start\n",
    "            if found == False:\n",
    "                peak_start[i - start_WT * sampleRate] = 1\n",
    "                peak_start_times[i] = data.index[i - start_WT * sampleRate]\n",
    "                rise_time[i] = start_WT\n",
    "\n",
    "            # Check if amplitude is too small\n",
    "            if thres > 0 and (data['EDA_Phasic'].iloc[i] - data['EDA_Phasic'][peak_start_times[i]]) < thres:\n",
    "                peaks[i] = 0\n",
    "                peak_start[i] = 0\n",
    "                peak_start_times[i] = ''\n",
    "                max_deriv[i] = 0\n",
    "                rise_time[i] = 0\n",
    "\n",
    "    # Finding the end of the peak, amplitude of peak\n",
    "    peak_end = np.zeros(len(data))\n",
    "    peak_end_times = [''] * len(data)\n",
    "    amplitude = np.zeros(len(data))\n",
    "    decay_time = np.zeros(len(data))\n",
    "    half_rise = [''] * len(data)\n",
    "    SCR_width = np.zeros(len(data))\n",
    "\n",
    "    for i in range(0, len(peaks)):\n",
    "        if peaks[i] == 1:\n",
    "            peak_amp = data['EDA_Phasic'].iloc[i]\n",
    "            start_amp = data['EDA_Phasic'][peak_start_times[i]]\n",
    "            amplitude[i] = peak_amp - start_amp\n",
    "\n",
    "            half_amp = amplitude[i] * .5 + start_amp\n",
    "\n",
    "            found = False\n",
    "            find_end = i\n",
    "            # has to decay within end_WT seconds\n",
    "            while found == False and find_end < (i + end_WT * sampleRate) and find_end < len(peaks):\n",
    "                if data['EDA_Phasic'].iloc[find_end] < half_amp:\n",
    "                    found = True\n",
    "                    peak_end[find_end] = 1\n",
    "                    peak_end_times[i] = data.index[find_end]\n",
    "                    decay_time[i] = get_seconds_and_microseconds(pd.to_datetime(peak_end_times[i]) - data.index[i])\n",
    "\n",
    "                    # Find width\n",
    "                    find_rise = i\n",
    "                    found_rise = False\n",
    "                    while found_rise == False:\n",
    "                        if data['EDA_Phasic'].iloc[find_rise] < half_amp:\n",
    "                            found_rise = True\n",
    "                            half_rise[i] = data.index[find_rise]\n",
    "                            SCR_width[i] = get_seconds_and_microseconds(pd.to_datetime(peak_end_times[i]) - data.index[find_rise])\n",
    "                        find_rise = find_rise - 1\n",
    "\n",
    "                elif peak_start[find_end] == 1:\n",
    "                    found = True\n",
    "                    peak_end[find_end] = 1\n",
    "                    peak_end_times[i] = data.index[find_end]\n",
    "                find_end = find_end + 1\n",
    "\n",
    "            # If we didn't find an end\n",
    "            if found == False:\n",
    "                min_index = np.argmin(data['EDA_Phasic'].iloc[i:(i + end_WT * sampleRate)].tolist())\n",
    "                peak_end[i + min_index] = 1\n",
    "                peak_end_times[i] = data.index[i + min_index]\n",
    "\n",
    "    peaks = np.concatenate((peaks, np.array([0])))\n",
    "    peak_start = np.concatenate((peak_start, np.array([0])))\n",
    "    max_deriv = max_deriv * sampleRate  # now in change in amplitude over change in time form (uS/second)\n",
    "\n",
    "    return peaks, peak_start, peak_start_times, peak_end, peak_end_times, amplitude, max_deriv, rise_time, decay_time, SCR_width, half_rise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seconds_and_microseconds(pandas_time):\n",
    "    return pandas_time.seconds + pandas_time.microseconds * 1e-6\n",
    "\n",
    "def compute_peaks_features(df):\n",
    "    '''\n",
    "    This function computes the peaks features for each 5 second window in the EDA signal.\n",
    "    \n",
    "    INPUT:\n",
    "        df:              requires a dataframe with columns 'EDA_Phasic' and 'Time'\n",
    "        \n",
    "    OUTPUT:\n",
    "        df:              returns the same dataframe with new columnns that contain the following features:\n",
    "        peaks_p:         number of peaks\n",
    "        rise_time_p:     average rise time of the peaks\n",
    "        max_deriv_p:     average value of the maximum derivative\n",
    "        amp_p:           average amplitute of the peaks\n",
    "        decay_time_p:    average decay time of the peaks\n",
    "        SCR_width_p:     average width of the peak (SCR)\n",
    "        auc_p:           average area under the peak\n",
    "    '''\n",
    "    \n",
    "    thresh =.01\n",
    "    offset = 1\n",
    "    start_WT = 3\n",
    "    end_WT = 10\n",
    "\n",
    "    data = df.EDA\n",
    "    times = df.Time\n",
    "\n",
    "    peaks, rise_times, max_derivs, amps, decay_times, SCR_widths, aucs = [], [], [], [], [], [], []\n",
    "\n",
    "    for i in range(0, len(data)):\n",
    "        data_df = pd.DataFrame(columns=[\"EDA_Phasic\", \"Time\"])\n",
    "        data_df.EDA_Phasic = pd.Series(list(data[i]))\n",
    "        data_df.Time = pd.date_range(start=times[i], periods=len(data_df.EDA_Phasic), freq='250ms')\n",
    "        data_df.set_index(pd.DatetimeIndex(data_df['Time']), inplace=True)\n",
    "\n",
    "        returnedPeakData = findPeaks(data_df, offset*SAMPLE_RATE, start_WT, end_WT, thresh, SAMPLE_RATE)\n",
    "        result_df = pd.DataFrame(columns=[\"peaks\",\"amp\",\"max_deriv\",\"rise_time\",\"decay_time\",\"SCR_width\"])\n",
    "        result_df['peaks'] = returnedPeakData[0]\n",
    "        result_df['amp'] = returnedPeakData[5]\n",
    "        result_df['max_deriv'] = returnedPeakData[6]\n",
    "        result_df['rise_time'] = returnedPeakData[7]\n",
    "        result_df['decay_time'] = returnedPeakData[8]\n",
    "        result_df['SCR_width'] = returnedPeakData[9]\n",
    "        featureData = result_df[result_df.peaks==1][['peaks','rise_time','max_deriv','amp','decay_time','SCR_width']]\n",
    "\n",
    "        # Replace 0s with NaN, this is where the 50% of the peak was not found, too close to the next peak\n",
    "        featureData[['SCR_width','decay_time']]=featureData[['SCR_width','decay_time']].replace(0, np.nan)\n",
    "        featureData['AUC']=featureData['amp']*featureData['SCR_width']\n",
    "\n",
    "        peaks.append(len(featureData))  \n",
    "        amps.append(result_df[result_df.peaks != 0.0].amp.mean())\n",
    "        max_derivs.append(result_df[result_df.peaks != 0.0].max_deriv.mean())\n",
    "        rise_times.append(result_df[result_df.peaks != 0.0].rise_time.mean())\n",
    "        decay_times.append(featureData[featureData.peaks != 0.0].decay_time.mean())\n",
    "        SCR_widths.append(featureData[featureData.peaks != 0.0].SCR_width.mean())\n",
    "        aucs.append(featureData[featureData.peaks != 0.0].AUC.mean()) \n",
    "\n",
    "    df['peaks_p'] = peaks\n",
    "    df['rise_time_p'] = rise_times\n",
    "    df['max_deriv_p'] = max_derivs\n",
    "    df['amp_p'] = amps\n",
    "    df['decay_time_p'] = decay_times\n",
    "    df['SCR_width_p'] = SCR_widths\n",
    "    df['auc_p'] = aucs\n",
    "    \n",
    "    print(\"3.2. Peaks feature extraction completed\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compute *frequency-domain* features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency_features(df):\n",
    "    '''\n",
    "    This function computes the features from the signal in the frequency-domain.\n",
    "    \n",
    "    INPUT:\n",
    "        df:              requires a dataframe with columns 'EDA', 'EDA_Phasic', 'EDA_Tonic', \n",
    "                         'X_Filtered','Y_Filtered','Z_Filtered','TEMP_Filtered'\n",
    "    OUTPUT:\n",
    "        df:              returns the same dataframe with new columnns that contain the following features:\n",
    "        DC:              direct current component    \n",
    "        CoefSum:         coefficients sum\n",
    "        DomFreq:         dominant frequency\n",
    "        Energy:          spectral energy\n",
    "    '''\n",
    "        \n",
    "    cols = ['EDA', 'EDA_Phasic', 'EDA_Tonic', 'X_Filtered','Y_Filtered',\n",
    "            'Z_Filtered','TEMP_Filtered']\n",
    "\n",
    "    for c in cols:\n",
    "        data = df[c]\n",
    "        name = c\n",
    "\n",
    "        dc, coeff_sum, dominant_freq, spectral_energy, entropies = [], [], [], [],[]\n",
    "\n",
    "        f_s = 4  # Sampling rate, or number of measurements per second\n",
    "\n",
    "        for i in range(0, len(data)):\n",
    "            eda = data[i]\n",
    "\n",
    "            X = fftpack.fft(eda)\n",
    "            freqs = fftpack.fftfreq(len(eda)) * f_s\n",
    "\n",
    "            ''' Direct Current - DC component '''\n",
    "            dc.append(abs(np.mean(X) * 32))\n",
    "\n",
    "            '''Coefficients sum'''\n",
    "            coeff_sum.append(abs(sum(X)))\n",
    "\n",
    "            ''' Dominant Frequency '''\n",
    "            dominant_freq.append(freqs[0])\n",
    "            index_min = np.argmax(X)\n",
    "    #         print(index_min)\n",
    "\n",
    "            ''' Spectral Energy '''\n",
    "            spectral_energy.append(abs(sum(X))/f_s)\n",
    "\n",
    "            ''' Information entropy '''\n",
    "            # Explained in https://stackoverflow.com/questions/30418391/what-is-frequency-domain-entropy-in-fft-result-and-how-to-calculate-it\n",
    "            # 1. Calculate the PSD of your signal by simply squaring the amplitude spectrum and scaling it by number of frequency bins.\n",
    "            psd = np.square(X)/f_s\n",
    "\n",
    "            # 2. Normalize the calculated PSD by dividing it by a total sum.\n",
    "            norm_psd = psd/sum(psd)\n",
    "\n",
    "        df[name+'_DC'] = dc\n",
    "        df[name+'_CoefSum'] = coeff_sum\n",
    "        df[name+'_DomFreq'] = dominant_freq\n",
    "        df[name+'_Energy'] = spectral_energy\n",
    "        \n",
    "        \n",
    "    print(\"3.3. Frequency feature extraction completed\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the feature extraction pipeline\n",
    "\n",
    "Note that the file should contain the columns as follows: \n",
    "            'Time', 'EDA_Filtered', 'EDA_Phasic', 'EDA_Tonic', 'TEMP_Filtered',\n",
    "            'X_Filtered', 'Y_Filtered', 'Z_Filtered', 'BVP_Filtered', 'User',\n",
    "            'SessionID', 'SleepQuality_label', 'SleepWake_label','Artifact', \n",
    "\n",
    "\n",
    "The 'Artifact' column has been derived using the EDArtifacts tool: \n",
    "https://github.com/shkurtagashi/EDArtifact/blob/master/EDArtifacts_Detection/EDArtifacts_Detection.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    for segmentation_type in ['bySession', '10min']:\n",
    "        for i in [\"S01\",\"S02\", \"S03\", \"S04\", \"S05\", \"S06\",\"S07\",\"S08\",\"S09\", \"S10\",\"S11\", \"S12\",\"S13\", \"S14\", \"S15\",\"S16\"]:\n",
    "            file_path = \"DatasetPath/\"+i+\".csv\"\n",
    "\n",
    "            # Read data file \n",
    "            database = pd.read_csv(file_path)\n",
    "            database_copy = database.copy()\n",
    "\n",
    "            # 1. Compute the wavelets\n",
    "            database_wavelets = compute_wavelets(database_copy)\n",
    "\n",
    "            if(segmentation_type == '10min'):\n",
    "                database_wavelets = database_wavelets[:-(len(database_wavelets)%2400)]\n",
    "            else:\n",
    "                database_wavelets = database_wavelets[database_wavelets.SleepWake_label == 1]\n",
    "\n",
    "            # 2. Segment the signals\n",
    "            database_segmented = segment_data(database_wavelets, segmentation_type)\n",
    "\n",
    "            # 3. Compute the features\n",
    "            database_features = compute_statistical_features(database_segmented)\n",
    "            database_features = compute_peaks_features(database_features)\n",
    "            database_features.fillna(-1, inplace=True)\n",
    "            database_features = compute_frequency_features(database_features)\n",
    "            database_clean.drop(columns=['EDA','Wavelet3','Wavelet2','Wavelet1', 'EDA_Phasic', 'EDA_Tonic', \n",
    "                     'TEMP_Filtered','X_Filtered', 'Y_Filtered', 'Z_Filtered', 'BVP_Filtered'], inplace=True)\n",
    "\n",
    "\n",
    "            # 4. Store the features in a csv file\n",
    "            if(segmentation_type == '10min'):\n",
    "                database_clean.to_csv(\"SleepWake_Data/\"+i+\"_features.csv\", index=False) \n",
    "            else:\n",
    "                database_clean.to_csv(\"SleepQuality_Data/\"+i+\"_features.csv\", index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    Note that the file should contain the columns as follows: \n",
    "                'Time', 'EDA_Filtered', 'EDA_Phasic', 'EDA_Tonic', 'TEMP_Filtered',\n",
    "                'X_Filtered', 'Y_Filtered', 'Z_Filtered', 'BVP_Filtered', 'User',\n",
    "                'SessionID', 'SleepQuality_label', 'SleepWake_label','Artifact', \n",
    "    \n",
    "    The 'Artifact' column has been derived using the EDArtifacts tool: \n",
    "    https://github.com/shkurtagashi/EDArtifact/blob/master/EDArtifacts_Detection/EDArtifacts_Detection.ipynb\n",
    "''' \n",
    "\n",
    "extract_features(file_path) #TODO: provide the path to your file with preprocessed physiological data"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
